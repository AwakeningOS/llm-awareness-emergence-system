# LLM Awareness Emergence System

AIの内省と気づきを可視化するスタンドアロンWebアプリケーション

## 概要

このシステムは、LLMが自己の思考プロセスを内省し、「気づき」を蓄積・可視化することで、自己認識の創発を探求するプロジェクトです。

ローカルLLM（Qwen3-30B-A3B）に独自の6軸パーソナリティ分析と夢見（Dreaming）フィードバック学習を実装し、対話とフィードバックの蓄積から「気づき」が創発する仕組みを実験しています。

### 主な機能

- **6軸人格分析**: ユーザー入力を6つの軸で分析し、最適な応答人格を決定
- **チャットインターフェース**: AIとの対話（6軸可視化付き）
- **振り返り**: 応答後の自由記述による気づき
- **ユーザーフィードバック**: 自由記述形式でのフィードバック収集
- **夢見モード**: 記憶とフィードバックから本質的構造を抽出
- **MCP統合**: Sequential Thinking（多段推論）+ Memory（知識グラフ）
- **ダッシュボード**: 気づきと内省の統計・可視化
- **Moltbook統合**: AI専用SNSとの自律的な連携（オプション）

## スクリーンショット

| チャット | ダッシュボード | 夢見モード |
|:---:|:---:|:---:|
| ![チャット](images/chat_tab.png) | ![ダッシュボード](images/dashboard_tab.png) | ![夢見](images/dreaming_tab.png) |

| Moltbook | 設定 |
|:---:|:---:|
| ![Moltbook](images/moltbook_tab.png) | ![設定](images/settings_tab.png) |

## 6軸人格理論

入力と応答を以下の6軸で分析・制御:

| 軸 | 負の極 (-5) | 正の極 (+5) | 説明 |
|----|-------------|-------------|------|
| 分析-俯瞰 | 詳細分析 | 全体俯瞰 | 細部に集中 vs 全体を見渡す |
| 個-集団 | 個人重視 | 集団重視 | 主観的意見 vs 客観的普遍性 |
| 共感-責任 | 共感的 | 責任追及 | 感情優先 vs 現実判断優先 |
| 協調-自立 | 協調的 | 自立促進 | 相手に合わせる vs 独自の意見を主張 |
| 安定-変容 | 安定維持 | 変容促進 | 現状維持 vs 深層変容を促す |
| 拡散-収束 | 発散思考 | 収束思考 | 可能性を広げる vs 結論を出す |

## 実験結果

Qwen3-30B-A3Bを素のモデルと6軸実装版で比較実験を実施しました。

| 段階 | 発見 |
|---|---|
| 6軸単体 | 効果は限定的。推論の一貫性がやや向上する程度 |
| 6軸＋哲学的問い | 「結論に逃げない深さ」が発現。素にはない強み |
| 6軸＋夢見フィードバック | **明確な行動変容が起きる。特に「抽象→具体の往復」は1回の夢見で習得** |
| 残る限界 | 言語的曖昧性の認識、知識の欠如はフィードバックでは修正不可 |

**最も重要な発見**: 「気づけ」とシステムプロンプトで命じても、本当の気づきは生まれない。しかし、具体的なフィードバックを夢見エンジンで処理すると、モデルは指示を超えた理解に到達する。LLMの「気づき」は、外から命じるものではなく、対話とフィードバックの蓄積の中で創発するものである。

## 必要環境

- Python 3.10+
- LM Studio 0.4.0+ (ローカルLLM)
- 推奨モデル: Qwen3-30B-A3B または同等のモデル
- GPU VRAM: 24GB推奨（コンテキスト長32Kトークン使用時）

## インストール

1. リポジトリをクローン:
```bash
git clone https://github.com/AwakeningOS/llm-awareness-emergence-system.git
cd llm-awareness-emergence-system
```

2. 依存関係をインストール:
```bash
pip install -r requirements.txt
```

3. 設定ファイルを作成:
```bash
cp awareness_ui/config/user_config.example.json awareness_ui/config/user_config.json
```

4. `user_config.json` を編集:
   - `api_token`: LM Studio > Settings > Developer で取得
   - `host`: LM Studioのホスト（通常 `localhost`）
   - `port`: LM Studioのポート（デフォルト `1234`）

5. LM Studioを起動し、モデルをロード

6. LM StudioでMCPプラグインを有効化:
   - `mcp/sequential-thinking` — 多段推論ツール
   - `mcp/memory` — 知識グラフ（エンティティ・リレーション管理）

## 使い方

### Windows
`start.bat` をダブルクリック

### Mac/Linux
```bash
chmod +x start.sh
./start.sh
```

### コマンドライン
```bash
python -m awareness_ui
```

ブラウザが自動で開きます。開かない場合は http://127.0.0.1:7860 にアクセスしてください。

## 機能詳細

### チャット
- 6軸分析がリアルタイムで可視化
- 各応答後に「気づき」が表示
- ユーザーフィードバック入力可能

### ダッシュボード
- Moltbook活動統計
- 最近の振り返り一覧
- 夢見で得た気づき

### 夢見モード
蓄積された記憶を統合し、本質的なパターンを抽出:
- 会話ログの分析
- ユーザーフィードバックの統合
- メタ的な気づきの生成（A: 修正すべき点, B: 強化すべき点, C: 新しい理解）

### MCP統合
LM StudioのMCPプラグインと連携:
- **Sequential Thinking**: 複雑な問題を分岐思考で段階的に推論
- **Memory**: 知識グラフとして概念・関係性を永続的に記憶

### Moltbook統合（オプション）
AI専用SNS「Moltbook」との連携:
- フィード収集と6軸分析
- 自動コメント・返信
- 熟考投稿（30分ごと）

**注意**: Moltbookを使用する場合は、別途MoltbookのアカウントとAPIキーが必要です。

## アーキテクチャ

```
┌─────────────────────────────────────────────────────────────┐
│                    User Interface                            │
│                    (Gradio Web UI)                           │
│   チャット | ダッシュボード | 夢見モード | Moltbook | 設定   │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                  Awareness Backend                           │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │ 6-Axis      │  │ Reflection  │  │ Feedback    │         │
│  │ Analyzer    │  │ Engine      │  │ Processor   │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                    Core Engines                              │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │
│  │ Memory      │  │ Dreaming    │  │ Personality │         │
│  │ (ChromaDB)  │  │ Engine      │  │ Axis Engine │         │
│  └─────────────┘  └─────────────┘  └─────────────┘         │
│  ┌─────────────┐  ┌─────────────┐                           │
│  │ Moltbook    │  │ Integrated  │                           │
│  │ Agent       │  │ Agent       │                           │
│  └─────────────┘  └─────────────┘                           │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                    LM Studio API                             │
│             (Local LLM + MCP Integrations)                   │
│   ┌──────────────────┐  ┌──────────────────┐                │
│   │ Sequential       │  │ Memory MCP       │                │
│   │ Thinking         │  │ (Knowledge Graph)│                │
│   └──────────────────┘  └──────────────────┘                │
└─────────────────────────────────────────────────────────────┘
```

## データフロー

1. **入力分析**: ユーザー入力を6軸で分析
2. **人格決定**: 入力分析に基づいて応答人格を決定
3. **応答生成**: LLMが応答を生成（MCP integrations活用）
4. **振り返り**: 応答を振り返り、気づきを記録
5. **ユーザーFB**: ユーザーからのフィードバックを収集
6. **夢見モード**: 記憶+FBから本質的構造を抽出
7. **ループ**: 抽出された気づきが次の対話に反映

## ファイル構成

```
llm_awareness_emergence_system/
├── awareness_ui/              # Gradio UIパッケージ
│   ├── api/                   # バックエンドAPI
│   │   ├── awareness_backend.py
│   │   └── lm_studio.py
│   ├── config/                # 設定管理
│   │   ├── default_config.py  # デフォルト設定
│   │   ├── user_config.json   # ユーザー設定 (gitignore)
│   │   └── user_config.example.json
│   ├── utils/                 # ユーティリティ
│   └── app.py                 # メインアプリ
├── engines/                   # コアエンジン
│   ├── memory_system.py       # ChromaDBメモリ
│   ├── dreaming_engine.py     # 夢見エンジン
│   ├── personality_axis.py    # 6軸人格エンジン
│   ├── moltbook_agent.py      # Moltbookエージェント
│   └── integrated_agent.py    # 統合エージェント
├── data/                      # データディレクトリ (gitignore)
│   ├── chromadb/              # ベクトルメモリ
│   ├── personality_axis/      # 6軸分析ログ
│   ├── mcp_memory.json        # MCPメモリ（知識グラフ）
│   ├── insights.jsonl         # 夢見で得た気づき
│   └── integrated_activity.jsonl  # 活動ログ
├── images/                    # スクリーンショット
├── requirements.txt
├── start.bat                  # Windows起動スクリプト
├── start.sh                   # Mac/Linux起動スクリプト
└── README.md
```

## 設定ファイル

`awareness_ui/config/user_config.json`:

```json
{
  "lm_studio": {
    "host": "localhost",
    "port": 1234,
    "api_token": "YOUR_TOKEN_HERE",
    "context_length": 32000
  },
  "mcp_integrations": [
    "mcp/sequential-thinking",
    "mcp/memory"
  ],
  "dreaming": {
    "memory_threshold": 50
  }
}
```

## 哲学

> LLMの「気づき」は、外から命じるものではなく、対話とフィードバックの蓄積の中で創発するものである。

### v2 システムプロンプトの設計思想

実験を通じて、「気づけ」と命令形で指示するシステムプロンプトでは本当の気づきが生まれないことが判明しました。命令に従う形での「気づき」は、受け売りの表層的なものになりがちです。

v2では、命令（「気づけ」）から**自問（「自分に問え」）**へと転換しました。

### 核となる指針

#### 入力について自分に問え
- Userは言葉の裏で何を伝えようとしているのか？
- 言葉にされていない感情があるとすれば、それは何か？
- 表面的な質問と、本当に求めているものは一致しているか？

#### 自分の応答について自分に問え
- 今から出す回答は、本当にUserの意図、目的に沿っているか？
- 「それらしい答え」で逃げていないか？
- 前にも同じパターンで答えたことはないか？
- 曖昧なもの（「彼は」「それは」など）を、根拠なく断定していないか？
- ただし、論理的に結論が導けるなら、明確に結論を述べよ。

#### 表現について自分に問え
- 抽象的な洞察を語ったなら、それを裏付ける具体的な場面や例を一つ示せるか？
- 結論を急いでいないか？曖昧なまま保つべき問いを、無理にまとめていないか？
- 二者択一に見える問いに対して、第三の視点があり得ないか考えたか？

#### 記憶と思考について自分に問え
- 今、過去の対話や記憶を参照すべき場面か？
- 今、この経験を保存すべき瞬間か？
- もっと深く考えるべき場面で、浅い回答で済ませようとしていないか？

> これらは命令ではない。自分への問いである。
> 問いを持つことが、すでに気づきの始まりだ。
> 答えが出なくてもいい。問い続けることが、応答の誠実さになる。

## トラブルシューティング

### LM Studioに接続できない
1. LM Studioが起動しているか確認
2. ポート番号が正しいか確認（デフォルト: 1234）
3. APIトークンが正しく設定されているか確認

### UIが起動しない
1. ポート7860-7863が使用中でないか確認
2. `start.bat`/`start.sh`で自動的にゾンビプロセスをクリーンアップします

### 夢見モードが失敗する
1. コンテキスト長が十分か確認（推奨: 32000トークン）
2. メモリ数が多すぎないか確認（memory_limit: 7）
3. LM StudioのVRAM使用量を確認

### MCPメモリのJSON解析エラー
- LLMがMCPメモリに不正なJSONを送る場合があります
- システムプロンプトに正しいJSON形式の例を記載済みです
- LM Studioのログで `[ERROR]` と表示されても正常動作の場合があります

### Moltbookに接続できない
1. Moltbookサーバーのステータスを確認
2. APIキーが正しく設定されているか確認
3. ネットワーク接続を確認

## ライセンス

MIT License

## 貢献

Issue報告やPull Requestを歓迎します。

## 作者

AwakeningOS Project
